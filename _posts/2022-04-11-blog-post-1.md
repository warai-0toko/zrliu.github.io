---
title: 'When does l1 minimization succeed for sparse signal recovery?'
date: 2022-04-11
permalink: /posts/2022/04/blog-post-1/
tags:
  - optimization
  - High-dimensional data analysis with low-dimensional models
---

Suppose the observed signal $y$ is generated from an unknown sparse signal $x_0$ by a linear mapping.
The goal of the sparse signal recovery problem is to recover the sparse signal $x_0$ from the observed signal $y$.
Specifically, it requires solving the following optimization problem:

$$
P_0
\begin{cases}
&\min ||x||_0 \\ 
& \text{s.t.} Ax=y 
\end{cases}
$$

where $x\in\mathbb{R}^m$ is the recovered signal, $y\in\mathbb{R}^n$ is the observed signal, and $A=[a_1|\cdots|a_n]\in\mathbb{R}^{m\times n}$ is the linear mapping.
<!-- Many real-world applications require to solve the sparse signal recovery problem.
For example, we want to recover the true image ($x$) from a corrupted one ($y$). -->
However, $P_0$ is NP-hard due to its combinational nature. 
In practice, we often relax the $L_0$ minimization to $L_1$ minimization as:

$$
P_1
\begin{cases}
&\min ||x||_1 \\ 
& \text{s.t.} Ax=y 
\end{cases}
$$

This post tries to informally introduce 
(1)  when solving $P_0$ is meaningful in the sense that its solution is exactly the true sparse signal $x_0$;
(2) when solving $P_1$ gives the exact solution to $P_0$.

## When the solution of $P_0$ is exactly $x_0$

Suppose the true sparse signal is $x_0$ with $||x_0||_0\leq k$, and the observed siginal $y$ is generated by $y=Ax_0=\sum_i a_i x_0(i)$.
Let $\hat{x}$ be the solution to $P_0$. In this section we analyze when $\hat{x} == x_0$.

Here we begin with the motivating analysis. 
The observed siginal $y$ equals $Ax_0=\sum_i a_i x_0(i)$.
Different columns of $A$ are mixed and we only have the access to the mixed results.
Intuitively, it is easier to guess which column participates in this linear combination of the columns of $A$ are orthogonal to each other.

Below we connect the above intuition to the sparse signal recovery problem.
Since $\hat{x}$ is the solution of $P_0$, we have $||\hat{x}||_0 \leq ||x_0||_0\leq k$.
Let $\delta = \hat{x} - x_0$ be the error signal.
We note that $\delta$ always lies in the null space of $A$ because $A\delta=A\hat{x}-Ax_0=0$.
We also notice that

$$
||\delta||_0 = ||\hat{x} - x_0||_0 \leq ||\hat{x}||_0 + ||x_0||_0\leq 2k
$$

To recover any sparse signal $x_0$ with $||x_0||_0\leq k$, it means that we have $\delta=\bf{0}$ for all $||x_0||_0\leq k$.
Mathmatically, it means *the only $\delta\in$ null($A$) with $||\delta||_0 \leq 2k$ is $\delta=\bf{0}$*. Or equalvently, every set of $2k$ columns of $A$ is linearly independent/orthognal.

Here we introduce the concept of Kruskal Rank: *The Kruskal Rank of a matrix A, written as Krank(A), is the largest number r such that every set of r columns of A is orthogonal*.
Based on the concept, below we formally summarize the above results:


($l_0$ minimization) Suppose that $y=Ax_0$ with $||x_0||\leq \frac{1}{2}$ krank($A$).

Then $x_o$ is the unique optimal solution to the $l_0$ minimization problem $P_0$


<!-- Here we introduce the *mutual coherence* $\mu(A)$ as -->

<!-- $$
\mu(A) = \max_{i\neq j}|\langle \frac{a_i}{||a_i||_2}, \frac{a_j}{||a_j||_2} \rangle|.
$$

We note that $0 \leq \mu(A) \leq 1$ and  $\mu(A)=0$ when $A$ is an orthogornal matrix. -->

<!-- In fact, if $\mu(A)$ is small, then the smallest sigular value $\delta_{\min}$ is not far from the largest sigular value $\delta_{\max}$.
To see this, we have
$A^\top A=I+\Delta$, where $\Delta_{i,j}=\langle a_i, a_j\rangle$ for $i\neq j$ and $\Delta_{i,i}=0$. We note that 

$$
||\Delta||_2 \leq ||\Delta||_F
$$ -->

## When solving $P_1$ gives the exact solution to $P_0$
